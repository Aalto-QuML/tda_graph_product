{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This document contains 3 implementations of computing the 0-dimensional PH of the product of 2 edge-based filtrations:\n",
        "\n",
        "1. Computing PH of the product directly using an union-find method.\n",
        "2. Theorem 5 of our paper.\n",
        "3. Computing PH of the product directly using the gudhi library.\n",
        "\n",
        "We then compute them on pairs of graphs loaded with the BREC dataset.\n",
        "\n",
        "NOTE: For convenience, we modeled time t = +infinity as -1 in the code file here, and the filtration values are assumed to be positive in the algorithm. We also precomputed the values at time t = -infinity appropriately."
      ],
      "metadata": {
        "id": "_k8ye8iWcXD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPNdbTzZkJUr"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install the gudhi package\n",
        "# !pip install gudhi\n",
        "\n",
        "import torch\n",
        "import itertools\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import random\n",
        "import gudhi\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code implementation of the Union Find PH Based Method\n",
        "# The code is adapted from codes produced in the paper:\n",
        "# \"Going beyond persistent homology using persistent homology\"\n",
        "# by Johanna Immonen, Amauri H. Souza, and Vikas Garg.\n",
        "\n",
        "class UnionFind():\n",
        "    def __init__(self,N):\n",
        "        self._parents = list(range(0, N))\n",
        "\n",
        "    def find(self, p):\n",
        "        return(self._parents[p])\n",
        "\n",
        "    def merge(self, p, q):\n",
        "        root_p, root_q = self._parents[p], self._parents[q]\n",
        "        for i in range(0, len(self._parents)):\n",
        "            if(self._parents[i] == root_p):\n",
        "                self._parents[i] = root_q\n",
        "\n",
        "    def connected(self,p,q):\n",
        "        return self._parents[p] == self._parents[q]\n",
        "\n",
        "    def roots(self):\n",
        "        roots = []\n",
        "        for i in range(0, len(self._parents)):\n",
        "            if self._parents[i] == i:\n",
        "                roots.append(i)\n",
        "        return roots\n",
        "\n",
        "\n",
        "def persistence_routine(filtered_v, filtered_e, edge_indices):\n",
        "\n",
        "    n, m = filtered_v.shape[0], edge_indices.shape[1]\n",
        "\n",
        "    filtered_e, sorted_indices = torch.sort(filtered_e)\n",
        "    uf = UnionFind(n)\n",
        "    persistence = torch.zeros((n, 2), device=filtered_v.device)\n",
        "    persistence1 = torch.zeros((m, 2), device=filtered_v.device)\n",
        "\n",
        "    unpaired_value = filtered_e[-1]  # used as infinity\n",
        "\n",
        "    for edge_index, edge_weight in zip(sorted_indices, filtered_e):\n",
        "\n",
        "        nodes = edge_indices[:, edge_index]\n",
        "        younger = uf.find(nodes[0])\n",
        "        older = uf.find(nodes[1])\n",
        "        if younger == older:\n",
        "            persistence1[edge_index, 0] = edge_weight # filtered_e[edge_index]\n",
        "            persistence1[edge_index, 1] = -1\n",
        "            continue\n",
        "        else:\n",
        "            if filtered_v[younger] < filtered_v[older]:\n",
        "                younger, older = older, younger\n",
        "                nodes = torch.flip(nodes, [0])\n",
        "\n",
        "        persistence[younger, 0] = filtered_v[younger]\n",
        "        persistence[younger, 1] = edge_weight\n",
        "\n",
        "        uf.merge(nodes[0], nodes[1])\n",
        "\n",
        "    for root in uf.roots():\n",
        "        persistence[root, 0] = filtered_v[root]\n",
        "        persistence[root, 1] = -1\n",
        "\n",
        "    return persistence, persistence1"
      ],
      "metadata": {
        "id": "20D71YqbkmGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Theorem 5 Implementation\n",
        "\n",
        "# Implementation of Theorem 5 of our paper.\n",
        "# Input: Takes in a graph data item of the form of a tuple\n",
        "# (graph_data_item 0, graph_data_item 1)\n",
        "\n",
        "# For i = 0 or 1, graph_data_item i is composed of 4 items for a graph G of n + 1 vertices with a vertex filtration function fv\n",
        "# The vertex set of the graph is given by {0, 1, ..., n}.\n",
        "# The 5 items in graph_data_item are:\n",
        "\n",
        "# input_v - a list of numbers that specifies the value of fv on the index i (this is not used in Theorem 5, they are always zero for edge filtrations)\n",
        "# input_e - a list of numbers that specifies the values of fe on the edge indices, ordered according to edge_index\n",
        "# edge_index - list of edges of the graph, being tuples of the form (i,j) for vertices i and j (made in pytorch)\n",
        "# filtered_v - torch.Tensor(input_v)\n",
        "# filtered_e = torch.Tensor(input_e)\n",
        "\n",
        "def thm_5_pd(graph_data_item):\n",
        "  input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G = graph_data_item[0]\n",
        "  input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H = graph_data_item[1]\n",
        "\n",
        "  # Precompute the Persistence Pairs using the Union Find Code Above\n",
        "  implG = persistence_routine(filtered_v_G, filtered_e_G, edge_index_G)[0]\n",
        "  implH = persistence_routine(filtered_v_H, filtered_e_H, edge_index_H)[0]\n",
        "\n",
        "  #  Both should have the same filtration steps, this uniformizes their time steps\n",
        "  filtration_steps = torch.unique(torch.cat((filtered_e_G, filtered_e_H))).tolist()\n",
        "  filtration_steps.sort()\n",
        "  filtration_steps.append(-1.0)\n",
        "  # print(filtration_steps)\n",
        "\n",
        "  # Initialize lists for number of vertices that dies non-trivially at each time\n",
        "  G_nd = [0]\n",
        "  H_nd = [0]\n",
        "  nG = filtered_v_G.shape[0]\n",
        "  nH = filtered_v_H.shape[0]\n",
        "\n",
        "  # Initialize lists for number of vertices still alive at each time\n",
        "  G_sa = [nG]\n",
        "  H_sa = [nH]\n",
        "\n",
        "  # Precompute the two respective lists\n",
        "  for i in range(0, len(filtration_steps)):\n",
        "    a_i = filtration_steps[i]\n",
        "\n",
        "    # Note for edge filtrations there are no trivial deaths of vertices\n",
        "    G_deaths = len([int(x[0].item()) for x in implG if x[1] == a_i])\n",
        "    H_deaths = len([int(x[0].item()) for x in implH if x[1] == a_i])\n",
        "\n",
        "    G_nd.append(G_deaths)\n",
        "    H_nd.append(H_deaths)\n",
        "\n",
        "    G_sa.append(G_sa[i] - G_deaths)\n",
        "    H_sa.append(H_sa[i] - H_deaths)\n",
        "\n",
        "  # Now we start computing 0-dim PH of the Product\n",
        "  persistence = []\n",
        "  for i in range(0, len(filtration_steps)):\n",
        "    a_i = filtration_steps[i]\n",
        "    hb_prev = H_sa[i]\n",
        "    gb_current = G_sa[i+1]\n",
        "\n",
        "    hd_current = H_nd[i+1]\n",
        "    gd_current = G_nd[i+1]\n",
        "\n",
        "    num = hb_prev*gd_current + gb_current*hd_current\n",
        "    persistence += [[0.0, a_i] for i in range(0, num)]\n",
        "\n",
        "  return persistence"
      ],
      "metadata": {
        "id": "ZQIe-KiyoYYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive PD List\n",
        "\n",
        "# Given a pair (i, j), thought of as a node in the product G \\Box H, and nH, the number of vertices in H\n",
        "# Returns the index (i, j) has if the matrix of vertices is flattened into a single list.\n",
        "def prod(pair, nH):\n",
        "  x = pair[0]\n",
        "  y = pair[1]\n",
        "  return nH*x + y\n",
        "\n",
        "# Naive implementation by computing PH of the whole product using the union-find structure\n",
        "# The inputs are - graph_data_item (explained in the implementation for Theorem 5)\n",
        "# P - the networkx graph of the product of the graph G and the graph H\n",
        "def naive_pd_edge(graph_data_item, P):\n",
        "  input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G = graph_data_item[0]\n",
        "  input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H = graph_data_item[1]\n",
        "\n",
        "  prod_vert_index = []\n",
        "  prod_vert_value = []\n",
        "  prod_edge_index = [[], []]\n",
        "  prod_edge_value = []\n",
        "\n",
        "  base_edge_index_G = np.transpose(edge_index_G.numpy()).tolist()\n",
        "  base_edge_index_H = np.transpose(edge_index_H.numpy()).tolist()\n",
        "  for item in base_edge_index_G:\n",
        "    item.sort()\n",
        "  for item in base_edge_index_H:\n",
        "    item.sort()\n",
        "\n",
        "  # print(base_edge_index_G)\n",
        "  # print(base_edge_index_H)\n",
        "  # print(\"-----------------\")\n",
        "\n",
        "  nG = filtered_v_G.shape[0]\n",
        "  nH = filtered_v_H.shape[0]\n",
        "\n",
        "  for i in range(0, nG):\n",
        "    for j in range(0, nH):\n",
        "      prod_vert_index.append((i,j))\n",
        "      prod_vert_value.append(0)\n",
        "  # print(len(prod_vert_index))\n",
        "  for v,w in P.edges:\n",
        "    prod_edge_index[0].append(prod(v, nH))\n",
        "    prod_edge_index[1].append(prod(w, nH))\n",
        "\n",
        "    if v[0] == w[0]:\n",
        "      input_index = base_edge_index_H.index([v[1],w[1]])\n",
        "      edge_value = input_e_H[input_index]\n",
        "    else:\n",
        "      input_index = base_edge_index_G.index([v[0],w[0]])\n",
        "      edge_value = input_e_G[input_index]\n",
        "    prod_edge_value.append(edge_value)\n",
        "\n",
        "  prod_edge_index_torch = torch.Tensor(prod_edge_index).long()\n",
        "  prod_filtered_v = torch.Tensor(prod_vert_value)\n",
        "  prod_filtered_e = torch.Tensor(prod_edge_value)\n",
        "\n",
        "  # Computes the PH on the entire product using the union-find code\n",
        "  prod_ph = persistence_routine(prod_filtered_v, prod_filtered_e, prod_edge_index_torch)[0].tolist()\n",
        "\n",
        "  return prod_ph"
      ],
      "metadata": {
        "id": "mY7XICpZulpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive implementation by computing PH of the whole product using the gudhi library\n",
        "# The inputs are - graph_data_item (explained in the implementation for Theorem 5)\n",
        "# P - the networkx graph of the product of the graph G and the graph H\n",
        "def gudhi_pd_edge(graph_data_item, P):\n",
        "  input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G = graph_data_item[0]\n",
        "  input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H = graph_data_item[1]\n",
        "\n",
        "  st = gudhi.SimplexTree()\n",
        "\n",
        "  prod_vert_index = []\n",
        "\n",
        "  base_edge_index_G = np.transpose(edge_index_G.numpy()).tolist()\n",
        "  base_edge_index_H = np.transpose(edge_index_H.numpy()).tolist()\n",
        "  for item in base_edge_index_G:\n",
        "    item.sort()\n",
        "  for item in base_edge_index_H:\n",
        "    item.sort()\n",
        "\n",
        "  # print(base_edge_index_G)\n",
        "  # print(base_edge_index_H)\n",
        "  # print(\"-----------------\")\n",
        "\n",
        "  nG = filtered_v_G.shape[0]\n",
        "  nH = filtered_v_H.shape[0]\n",
        "\n",
        "  for i in range(0, nG):\n",
        "    for j in range(0, nH):\n",
        "      prod_vert_index.append((i,j))\n",
        "      st.insert([prod((i,j), nH)], filtration=0)\n",
        "\n",
        "  for v,w in P.edges:\n",
        "    if v[0] == w[0]:\n",
        "      input_index = base_edge_index_H.index([v[1],w[1]])\n",
        "      edge_value = input_e_H[input_index]\n",
        "    else:\n",
        "      input_index = base_edge_index_G.index([v[0],w[0]])\n",
        "      edge_value = input_e_G[input_index]\n",
        "    st.insert([prod(v, nH), prod(w, nH)], filtration=edge_value)\n",
        "\n",
        "  st.make_filtration_non_decreasing()\n",
        "  dgms = st.persistence(min_persistence=-1)\n",
        "\n",
        "  dgms_no_dim = [list(x[1]) for x in dgms]\n",
        "  for item in dgms_no_dim:\n",
        "    _, d = item\n",
        "    if d == np.inf:\n",
        "      item[1] = -1.0\n",
        "  return dgms_no_dim\n",
        "\n"
      ],
      "metadata": {
        "id": "umD8VnMT1Ugt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest of the Code does a comparison testing between the 3 implementations to see that they agree with each other. We take the BREC dataset (see the paper for specifications) and compute the 0-th dim PH of the product of pair of graphs in the BREC dataset, using an augmented Forman-Ricci-curvature filtration on the two components."
      ],
      "metadata": {
        "id": "FlF6jVMhUgfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def get_dataset(name):\n",
        "    dataset = []\n",
        "    if name in ['basic', 'str', 'dr', '4vtx']:\n",
        "        # When you are running this, replace the following line with a link to the relevant file in the BREC dataset.\n",
        "        data = np.load(f'/content/drive/My Drive/Colab Notebooks/datasets/BREC/{name}.npy')\n",
        "        for i in range(0, data.size, 2):\n",
        "            if name == 'dr':\n",
        "                pyg_graph_1 =  nx.from_graph6_bytes(data[i])\n",
        "                pyg_graph_2 =  nx.from_graph6_bytes(data[i+1])\n",
        "            else:\n",
        "                pyg_graph_1 =  nx.from_graph6_bytes(data[i].encode())\n",
        "                pyg_graph_2 =  nx.from_graph6_bytes(data[i+1].encode())\n",
        "            dataset.append((pyg_graph_1, pyg_graph_2))\n",
        "        return dataset\n",
        "    elif name in ['regular', 'extension', 'cfi']:\n",
        "        # When you are running this, replace the following line with a link to the relevant file in the BREC dataset.\n",
        "        data = np.load(f'/content/drive/My Drive/Colab Notebooks/datasets/BREC/{name}.npy')\n",
        "        for i in range(0, data.size // 2):\n",
        "            g6_tuple = data[i]\n",
        "            if name == 'regular' or name == 'cfi':\n",
        "                pyg_graph_1 = nx.from_graph6_bytes(g6_tuple[0])\n",
        "                pyg_graph_2 = nx.from_graph6_bytes(g6_tuple[1])\n",
        "            else:\n",
        "                pyg_graph_1 = nx.from_graph6_bytes(g6_tuple[0].encode())\n",
        "                pyg_graph_2 = nx.from_graph6_bytes(g6_tuple[1].encode())\n",
        "            dataset.append((pyg_graph_1, pyg_graph_2))\n",
        "        return dataset\n",
        "\n",
        "dataset = get_dataset('basic')\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCNlt_0iVcV2",
        "outputId": "eb65af42-9e85-4263-e105-9222fd6d850a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the dataset inputs to entries for the 3 algorithms\n",
        "# Here we filtrate the two graphs using an augmented Forman-Ricci curvature (as edge filtrations)\n",
        "\n",
        "def dataset_entry_to_input(item):\n",
        "  G, H = item\n",
        "\n",
        "  G_edge_list = [[], []]\n",
        "  H_edge_list = [[], []]\n",
        "  for v0, v1 in G.edges:\n",
        "    G_edge_list[0].append(v0)\n",
        "    G_edge_list[1].append(v1)\n",
        "\n",
        "  for v0, v1 in H.edges:\n",
        "    H_edge_list[0].append(v0)\n",
        "    H_edge_list[1].append(v1)\n",
        "\n",
        "  G_nodes = list(G.nodes)\n",
        "  H_nodes = list(H.nodes)\n",
        "\n",
        "  G_edges = list(G.edges)\n",
        "  H_edges = list(H.edges)\n",
        "\n",
        "  input_v_G = [0 for _ in range(0, len(G_nodes))]\n",
        "  input_v_H = [0 for _ in range(0, len(H_nodes))]\n",
        "\n",
        "  input_e_G = []\n",
        "  input_e_H = []\n",
        "\n",
        "  # augmented Formanâ€“Ricci curvatures\n",
        "  for u, w in G_edges:\n",
        "    u_neighbors = list(G.neighbors(u))\n",
        "    w_neighbors = list(G.neighbors(w))\n",
        "    intersection = list(set(u_neighbors).intersection(set(w_neighbors)))\n",
        "    input_e_G.append(4 - len(u_neighbors) - len(w_neighbors) + 3*len(intersection) + 2*len(G_nodes))\n",
        "  for u, w in H_edges:\n",
        "    u_neighbors = list(H.neighbors(u))\n",
        "    w_neighbors = list(H.neighbors(w))\n",
        "    intersection = list(set(u_neighbors).intersection(set(w_neighbors)))\n",
        "    input_e_H.append(4 - len(u_neighbors) - len(w_neighbors) + 3*len(intersection) + 2*len(H_nodes))\n",
        "\n",
        "  edge_index_G = torch.Tensor(G_edge_list).long()\n",
        "  filtered_v_G = torch.Tensor(input_v_G)\n",
        "  filtered_e_G = torch.Tensor(input_e_G)\n",
        "\n",
        "  edge_index_H = torch.Tensor(H_edge_list).long()\n",
        "  filtered_v_H = torch.Tensor(input_v_H)\n",
        "  filtered_e_H = torch.Tensor(input_e_H)\n",
        "\n",
        "  return ((input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G), (input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H))\n",
        "\n",
        "\n",
        "graph_data_list = []\n",
        "for item in dataset:\n",
        "  entry_gd = dataset_entry_to_input(item)\n",
        "  graph_data_list.append(entry_gd)"
      ],
      "metadata": {
        "id": "U2Rty5ISVVTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the results for the 3 implementations\n",
        "\n",
        "# Theorem 5\n",
        "thm_5_pd_list = []\n",
        "for gditem in graph_data_list:\n",
        "  thm_5_pd_list.append(thm_5_pd(gditem))\n",
        "\n",
        "# Compute the actual NetworkX graph of the product of the two graphs\n",
        "P_list = []\n",
        "for item in dataset:\n",
        "  G, H = item\n",
        "  P = nx.cartesian_product(G, H)\n",
        "  P_list.append(P)\n",
        "\n",
        "# Naive Union-Find\n",
        "naive_pd_e_list = []\n",
        "for ind in range(0, len(graph_data_list)):\n",
        "  gditem = graph_data_list[ind]\n",
        "  naive_pd_e_list.append(naive_pd_edge(gditem, P_list[ind]))\n",
        "\n",
        "# gudhi\n",
        "gudhi_pd_e_list = []\n",
        "for ind in range(0, len(graph_data_list)):\n",
        "  gditem = graph_data_list[ind]\n",
        "  gudhi_pd_e_list.append(gudhi_pd_edge(gditem, P_list[ind]))"
      ],
      "metadata": {
        "id": "pMwDkwH9ULKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions to check if two persistence diagrams are equal\n",
        "def pd_to_multiset(pd_list):\n",
        "  output = {}\n",
        "  for item in pd_list:\n",
        "    output[str(item)] = 0\n",
        "  for item in pd_list:\n",
        "    output[str(item)] += 1\n",
        "  return output\n",
        "\n",
        "def check_pd_equal(pd1, pd2):\n",
        "  pd1_list = pd_to_multiset(pd1)\n",
        "  pd2_list = pd_to_multiset(pd2)\n",
        "\n",
        "  return pd1_list, pd2_list, pd1_list == pd2_list"
      ],
      "metadata": {
        "id": "OXNWZQ7r1sky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking that the outputs match each other\n",
        "for i in range(0, len(thm_5_pd_list)):\n",
        "  thm4_output = thm_5_pd_list[i]\n",
        "  naive_output = naive_pd_e_list[i]\n",
        "  gudhi_output = gudhi_pd_e_list[i]\n",
        "\n",
        "  left_list, middle_list, value = check_pd_equal(thm4_output, naive_output)\n",
        "  middle_list, right_list, value1 = check_pd_equal(naive_output, gudhi_output)\n",
        "\n",
        "  if value and value1:\n",
        "    print(True)\n",
        "  else:\n",
        "    print(\"False: \", i)\n",
        "    print(\"Thm4 = Naive\", value)\n",
        "    print(\"Naive = gudhi\", value1)\n",
        "    print(left_list)\n",
        "    print(middle_list)\n",
        "    print(right_list)\n",
        "    print(\"--------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTTdIOEg2vqc",
        "outputId": "83f4acf7-2086-489d-d72d-b89669b66307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    }
  ]
}