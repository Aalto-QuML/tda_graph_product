{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This document contains 3 implementations of computing the 0-dimensional PH of the product of 2 edge-based filtrations:\n",
        "\n",
        "1. Computing PH of the product directly using an union-find method.\n",
        "2. Theorem 5 of our paper.\n",
        "3. Computing PH of the product directly using the gudhi library.\n",
        "\n",
        "We then assess their runtime performance on pairs of graphs loaded with (all of) the BREC dataset.\n",
        "\n",
        "NOTE: For convenience, we modeled time t = +infinity as -1 in the code file here, and the filtration values are assumed to be positive in the algorithm. We also precomputed the values at time t = -infinity appropriately."
      ],
      "metadata": {
        "id": "_k8ye8iWcXD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lPNdbTzZkJUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a94431-8614-43f3-f9f9-608aa3c43202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gudhi in /usr/local/lib/python3.12/dist-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from gudhi) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install the gudhi package\n",
        "!pip install gudhi\n",
        "\n",
        "import torch\n",
        "import itertools\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import random\n",
        "import gudhi\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code implementation of the Union Find PH Based Method\n",
        "# The code is adapted from codes produced in the paper:\n",
        "# \"Going beyond persistent homology using persistent homology\"\n",
        "# by Johanna Immonen, Amauri H. Souza, and Vikas Garg.\n",
        "\n",
        "class UnionFind():\n",
        "    def __init__(self,N):\n",
        "        self._parents = list(range(0, N))\n",
        "\n",
        "    def find(self, p):\n",
        "        return(self._parents[p])\n",
        "\n",
        "    def merge(self, p, q):\n",
        "        root_p, root_q = self._parents[p], self._parents[q]\n",
        "        for i in range(0, len(self._parents)):\n",
        "            if(self._parents[i] == root_p):\n",
        "                self._parents[i] = root_q\n",
        "\n",
        "    def connected(self,p,q):\n",
        "        return self._parents[p] == self._parents[q]\n",
        "\n",
        "    def roots(self):\n",
        "        roots = []\n",
        "        for i in range(0, len(self._parents)):\n",
        "            if self._parents[i] == i:\n",
        "                roots.append(i)\n",
        "        return roots\n",
        "\n",
        "\n",
        "def persistence_routine(filtered_v, filtered_e, edge_indices):\n",
        "\n",
        "    n, m = filtered_v.shape[0], edge_indices.shape[1]\n",
        "\n",
        "    filtered_e, sorted_indices = torch.sort(filtered_e)\n",
        "    uf = UnionFind(n)\n",
        "    persistence = torch.zeros((n, 2), device=filtered_v.device)\n",
        "    persistence1 = torch.zeros((m, 2), device=filtered_v.device)\n",
        "\n",
        "    unpaired_value = filtered_e[-1]  # used as infinity\n",
        "\n",
        "    for edge_index, edge_weight in zip(sorted_indices, filtered_e):\n",
        "\n",
        "        nodes = edge_indices[:, edge_index]\n",
        "        younger = uf.find(nodes[0])\n",
        "        older = uf.find(nodes[1])\n",
        "        if younger == older:\n",
        "            persistence1[edge_index, 0] = edge_weight # filtered_e[edge_index]\n",
        "            persistence1[edge_index, 1] = -1\n",
        "            continue\n",
        "        else:\n",
        "            if filtered_v[younger] < filtered_v[older]:\n",
        "                younger, older = older, younger\n",
        "                nodes = torch.flip(nodes, [0])\n",
        "\n",
        "        persistence[younger, 0] = filtered_v[younger]\n",
        "        persistence[younger, 1] = edge_weight\n",
        "\n",
        "        uf.merge(nodes[0], nodes[1])\n",
        "\n",
        "    for root in uf.roots():\n",
        "        persistence[root, 0] = filtered_v[root]\n",
        "        persistence[root, 1] = -1\n",
        "\n",
        "    return persistence, persistence1"
      ],
      "metadata": {
        "id": "20D71YqbkmGp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Theorem 5 Implementation\n",
        "\n",
        "# Implementation of Theorem 5 of our paper.\n",
        "# Input: Takes in a graph data item of the form of a tuple\n",
        "# (graph_data_item 0, graph_data_item 1)\n",
        "\n",
        "# For i = 0 or 1, graph_data_item i is composed of 4 items for a graph G of n + 1 vertices with a vertex filtration function fv\n",
        "# The vertex set of the graph is given by {0, 1, ..., n}.\n",
        "# The 5 items in graph_data_item are:\n",
        "\n",
        "# input_v - a list of numbers that specifies the value of fv on the index i (this is not used in Theorem 5, they are always zero for edge filtrations)\n",
        "# input_e - a list of numbers that specifies the values of fe on the edge indices, ordered according to edge_index\n",
        "# edge_index - list of edges of the graph, being tuples of the form (i,j) for vertices i and j (made in pytorch)\n",
        "# filtered_v - torch.Tensor(input_v)\n",
        "# filtered_e = torch.Tensor(input_e)\n",
        "\n",
        "def thm_5_pd(graph_data_item):\n",
        "  input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G = graph_data_item[0]\n",
        "  input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H = graph_data_item[1]\n",
        "\n",
        "  # Precompute the Persistence Pairs using the Union Find Code Above\n",
        "  implG = persistence_routine(filtered_v_G, filtered_e_G, edge_index_G)[0]\n",
        "  implH = persistence_routine(filtered_v_H, filtered_e_H, edge_index_H)[0]\n",
        "\n",
        "  #  Both should have the same filtration steps, this uniformizes their time steps\n",
        "  filtration_steps = torch.unique(torch.cat((filtered_e_G, filtered_e_H))).tolist()\n",
        "  filtration_steps.sort()\n",
        "  filtration_steps.append(-1.0)\n",
        "  # print(filtration_steps)\n",
        "\n",
        "  # Initialize lists for number of vertices that dies non-trivially at each time\n",
        "  G_nd = [0]\n",
        "  H_nd = [0]\n",
        "  nG = filtered_v_G.shape[0]\n",
        "  nH = filtered_v_H.shape[0]\n",
        "\n",
        "  # Initialize lists for number of vertices still alive at each time\n",
        "  G_sa = [nG]\n",
        "  H_sa = [nH]\n",
        "\n",
        "  # Precompute the two respective lists\n",
        "  for i in range(0, len(filtration_steps)):\n",
        "    a_i = filtration_steps[i]\n",
        "\n",
        "    # Note for edge filtrations there are no trivial deaths of vertices\n",
        "    G_deaths = len([int(x[0].item()) for x in implG if x[1] == a_i])\n",
        "    H_deaths = len([int(x[0].item()) for x in implH if x[1] == a_i])\n",
        "\n",
        "    G_nd.append(G_deaths)\n",
        "    H_nd.append(H_deaths)\n",
        "\n",
        "    G_sa.append(G_sa[i] - G_deaths)\n",
        "    H_sa.append(H_sa[i] - H_deaths)\n",
        "\n",
        "  # Now we start computing 0-dim PH of the Product\n",
        "  persistence = []\n",
        "  for i in range(0, len(filtration_steps)):\n",
        "    a_i = filtration_steps[i]\n",
        "    hb_prev = H_sa[i]\n",
        "    gb_current = G_sa[i+1]\n",
        "\n",
        "    hd_current = H_nd[i+1]\n",
        "    gd_current = G_nd[i+1]\n",
        "\n",
        "    num = hb_prev*gd_current + gb_current*hd_current\n",
        "    persistence += [[0.0, a_i] for i in range(0, num)]\n",
        "\n",
        "  return persistence"
      ],
      "metadata": {
        "id": "ZQIe-KiyoYYc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive PD List\n",
        "\n",
        "# Given a pair (i, j), thought of as a node in the product G \\Box H, and nH, the number of vertices in H\n",
        "# Returns the index (i, j) has if the matrix of vertices is flattened into a single list.\n",
        "def prod(pair, nH):\n",
        "  x = pair[0]\n",
        "  y = pair[1]\n",
        "  return nH*x + y\n",
        "\n",
        "# Naive implementation by computing PH of the whole product using the union-find structure\n",
        "# The inputs are - graph_data_item (explained in the implementation for Theorem 5)\n",
        "# P - the networkx graph of the product of the graph G and the graph H\n",
        "def naive_pd_edge(graph_data_item, P):\n",
        "  input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G = graph_data_item[0]\n",
        "  input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H = graph_data_item[1]\n",
        "\n",
        "  prod_vert_index = []\n",
        "  prod_vert_value = []\n",
        "  prod_edge_index = [[], []]\n",
        "  prod_edge_value = []\n",
        "\n",
        "  base_edge_index_G = np.transpose(edge_index_G.numpy()).tolist()\n",
        "  base_edge_index_H = np.transpose(edge_index_H.numpy()).tolist()\n",
        "  for item in base_edge_index_G:\n",
        "    item.sort()\n",
        "  for item in base_edge_index_H:\n",
        "    item.sort()\n",
        "\n",
        "  # print(base_edge_index_G)\n",
        "  # print(base_edge_index_H)\n",
        "  # print(\"-----------------\")\n",
        "\n",
        "  nG = filtered_v_G.shape[0]\n",
        "  nH = filtered_v_H.shape[0]\n",
        "\n",
        "  for i in range(0, nG):\n",
        "    for j in range(0, nH):\n",
        "      prod_vert_index.append((i,j))\n",
        "      prod_vert_value.append(0)\n",
        "  # print(len(prod_vert_index))\n",
        "  for v,w in P.edges:\n",
        "    prod_edge_index[0].append(prod(v, nH))\n",
        "    prod_edge_index[1].append(prod(w, nH))\n",
        "\n",
        "    if v[0] == w[0]:\n",
        "      input_index = base_edge_index_H.index([v[1],w[1]])\n",
        "      edge_value = input_e_H[input_index]\n",
        "    else:\n",
        "      input_index = base_edge_index_G.index([v[0],w[0]])\n",
        "      edge_value = input_e_G[input_index]\n",
        "    prod_edge_value.append(edge_value)\n",
        "\n",
        "  prod_edge_index_torch = torch.Tensor(prod_edge_index).long()\n",
        "  prod_filtered_v = torch.Tensor(prod_vert_value)\n",
        "  prod_filtered_e = torch.Tensor(prod_edge_value)\n",
        "\n",
        "  # Computes the PH on the entire product using the union-find code\n",
        "  prod_ph = persistence_routine(prod_filtered_v, prod_filtered_e, prod_edge_index_torch)[0].tolist()\n",
        "\n",
        "  return prod_ph"
      ],
      "metadata": {
        "id": "mY7XICpZulpb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive implementation by computing PH of the whole product using the gudhi library\n",
        "# The inputs are - graph_data_item (explained in the implementation for Theorem 5)\n",
        "# P - the networkx graph of the product of the graph G and the graph H\n",
        "def gudhi_pd_edge(graph_data_item, P):\n",
        "  input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G = graph_data_item[0]\n",
        "  input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H = graph_data_item[1]\n",
        "\n",
        "  st = gudhi.SimplexTree()\n",
        "\n",
        "  prod_vert_index = []\n",
        "\n",
        "  base_edge_index_G = np.transpose(edge_index_G.numpy()).tolist()\n",
        "  base_edge_index_H = np.transpose(edge_index_H.numpy()).tolist()\n",
        "  for item in base_edge_index_G:\n",
        "    item.sort()\n",
        "  for item in base_edge_index_H:\n",
        "    item.sort()\n",
        "\n",
        "  # print(base_edge_index_G)\n",
        "  # print(base_edge_index_H)\n",
        "  # print(\"-----------------\")\n",
        "\n",
        "  nG = filtered_v_G.shape[0]\n",
        "  nH = filtered_v_H.shape[0]\n",
        "\n",
        "  for i in range(0, nG):\n",
        "    for j in range(0, nH):\n",
        "      prod_vert_index.append((i,j))\n",
        "      st.insert([prod((i,j), nH)], filtration=0)\n",
        "\n",
        "  for v,w in P.edges:\n",
        "    if v[0] == w[0]:\n",
        "      input_index = base_edge_index_H.index([v[1],w[1]])\n",
        "      edge_value = input_e_H[input_index]\n",
        "    else:\n",
        "      input_index = base_edge_index_G.index([v[0],w[0]])\n",
        "      edge_value = input_e_G[input_index]\n",
        "    st.insert([prod(v, nH), prod(w, nH)], filtration=edge_value)\n",
        "\n",
        "  st.make_filtration_non_decreasing()\n",
        "  dgms = st.persistence(min_persistence=-1)\n",
        "\n",
        "  dgms_no_dim = [list(x[1]) for x in dgms]\n",
        "  for item in dgms_no_dim:\n",
        "    _, d = item\n",
        "    if d == np.inf:\n",
        "      item[1] = -1.0\n",
        "  return dgms_no_dim\n",
        "\n"
      ],
      "metadata": {
        "id": "umD8VnMT1Ugt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest of the Code computes the runtime of all 3 implementations on each dataset in the BREC dataset.\n",
        "\n",
        "We take the BREC dataset (see the paper for specifications) and compute the 0-th dim PH of the product of pair of graphs in the BREC dataset, using an augmented Forman-Ricci-curvature filtration on the two components."
      ],
      "metadata": {
        "id": "FlF6jVMhUgfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def get_dataset(name):\n",
        "    dataset = []\n",
        "    if name in ['basic', 'str', 'dr', '4vtx']:\n",
        "        # When you are running this, replace the following line with a link to the relevant file in the BREC dataset.\n",
        "        data = np.load(f'/content/drive/My Drive/Colab Notebooks/datasets/BREC/{name}.npy')\n",
        "        for i in range(0, data.size, 2):\n",
        "            if name == 'dr':\n",
        "                pyg_graph_1 =  nx.from_graph6_bytes(data[i])\n",
        "                pyg_graph_2 =  nx.from_graph6_bytes(data[i+1])\n",
        "            else:\n",
        "                pyg_graph_1 =  nx.from_graph6_bytes(data[i].encode())\n",
        "                pyg_graph_2 =  nx.from_graph6_bytes(data[i+1].encode())\n",
        "            dataset.append((pyg_graph_1, pyg_graph_2))\n",
        "        return dataset\n",
        "    elif name in ['regular', 'extension', 'cfi']:\n",
        "        # When you are running this, replace the following line with a link to the relevant file in the BREC dataset.\n",
        "        data = np.load(f'/content/drive/My Drive/Colab Notebooks/datasets/BREC/{name}.npy')\n",
        "        for i in range(0, data.size // 2):\n",
        "            g6_tuple = data[i]\n",
        "            if name == 'regular' or name == 'cfi':\n",
        "                pyg_graph_1 = nx.from_graph6_bytes(g6_tuple[0])\n",
        "                pyg_graph_2 = nx.from_graph6_bytes(g6_tuple[1])\n",
        "            else:\n",
        "                pyg_graph_1 = nx.from_graph6_bytes(g6_tuple[0].encode())\n",
        "                pyg_graph_2 = nx.from_graph6_bytes(g6_tuple[1].encode())\n",
        "            dataset.append((pyg_graph_1, pyg_graph_2))\n",
        "        return dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCNlt_0iVcV2",
        "outputId": "b0971cb8-4e9f-4af1-e8cd-3bddb7088294"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the dataset inputs to entries for the 3 algorithms\n",
        "# Here we filtrate the two graphs using an augmented Forman-Ricci curvature (as edge filtrations)\n",
        "\n",
        "def dataset_entry_to_input(item):\n",
        "  G, H = item\n",
        "\n",
        "  G_edge_list = [[], []]\n",
        "  H_edge_list = [[], []]\n",
        "  for v0, v1 in G.edges:\n",
        "    G_edge_list[0].append(v0)\n",
        "    G_edge_list[1].append(v1)\n",
        "\n",
        "  for v0, v1 in H.edges:\n",
        "    H_edge_list[0].append(v0)\n",
        "    H_edge_list[1].append(v1)\n",
        "\n",
        "  G_nodes = list(G.nodes)\n",
        "  H_nodes = list(H.nodes)\n",
        "\n",
        "  G_edges = list(G.edges)\n",
        "  H_edges = list(H.edges)\n",
        "\n",
        "  input_v_G = [0 for _ in range(0, len(G_nodes))]\n",
        "  input_v_H = [0 for _ in range(0, len(H_nodes))]\n",
        "\n",
        "  input_e_G = []\n",
        "  input_e_H = []\n",
        "\n",
        "  # augmented Formanâ€“Ricci curvatures\n",
        "  for u, w in G_edges:\n",
        "    u_neighbors = list(G.neighbors(u))\n",
        "    w_neighbors = list(G.neighbors(w))\n",
        "    intersection = list(set(u_neighbors).intersection(set(w_neighbors)))\n",
        "    input_e_G.append(4 - len(u_neighbors) - len(w_neighbors) + 3*len(intersection) + 2*len(G_nodes))\n",
        "  for u, w in H_edges:\n",
        "    u_neighbors = list(H.neighbors(u))\n",
        "    w_neighbors = list(H.neighbors(w))\n",
        "    intersection = list(set(u_neighbors).intersection(set(w_neighbors)))\n",
        "    input_e_H.append(4 - len(u_neighbors) - len(w_neighbors) + 3*len(intersection) + 2*len(H_nodes))\n",
        "\n",
        "  edge_index_G = torch.Tensor(G_edge_list).long()\n",
        "  filtered_v_G = torch.Tensor(input_v_G)\n",
        "  filtered_e_G = torch.Tensor(input_e_G)\n",
        "\n",
        "  edge_index_H = torch.Tensor(H_edge_list).long()\n",
        "  filtered_v_H = torch.Tensor(input_v_H)\n",
        "  filtered_e_H = torch.Tensor(input_e_H)\n",
        "\n",
        "  return ((input_v_G, input_e_G, edge_index_G, filtered_v_G, filtered_e_G), (input_v_H, input_e_H, edge_index_H, filtered_v_H, filtered_e_H))"
      ],
      "metadata": {
        "id": "U2Rty5ISVVTx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Dataset Entries for All of Them\n",
        "# dataset_names = ['basic', 'str']\n",
        "dataset_names = ['basic', 'str', 'dr', '4vtx', 'regular', 'extension', 'cfi']\n",
        "\n",
        "raw_data_list = []\n",
        "graph_data_list = []\n",
        "for name in dataset_names:\n",
        "  dataset = get_dataset(name)\n",
        "  print(name, \": \", len(dataset))\n",
        "\n",
        "  data_list = []\n",
        "  for item in dataset:\n",
        "    entry_gd = dataset_entry_to_input(item)\n",
        "    data_list.append(entry_gd)\n",
        "\n",
        "  raw_data_list.append(dataset)\n",
        "  graph_data_list.append(data_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5W9TnlCcvYv",
        "outputId": "5222eec3-0100-49a3-9a76-29f42c38beb1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic :  60\n",
            "str :  50\n",
            "dr :  20\n",
            "4vtx :  20\n",
            "regular :  50\n",
            "extension :  100\n",
            "cfi :  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the results for the 3 implementations\n",
        "thm5_time_list = []\n",
        "naive_time_list = []\n",
        "gudhi_time_list = []\n",
        "\n",
        "thm_5_pd_list = []\n",
        "naive_pd_e_list = []\n",
        "gudhi_pd_e_list = []\n",
        "\n",
        "for i in range(0, len(dataset_names)):\n",
        "  data_list = graph_data_list[i]\n",
        "  dataset = raw_data_list[i]\n",
        "\n",
        "  # Theorem 5\n",
        "  thm5_t = time.time()\n",
        "  for gditem in data_list:\n",
        "    thm_5_pd_list.append(thm_5_pd(gditem))\n",
        "  thm5_time = time.time() - thm5_t\n",
        "  thm5_time_list.append(thm5_time)\n",
        "\n",
        "  # Compute the actual NetworkX graph of the product of the two graphs\n",
        "  P_list = []\n",
        "  for item in dataset:\n",
        "    G, H = item\n",
        "    P = nx.cartesian_product(G, H)\n",
        "    P_list.append(P)\n",
        "\n",
        "  # Naive Union-Find\n",
        "  naive_t = time.time()\n",
        "  for ind in range(0, len(data_list)):\n",
        "    gditem = data_list[ind]\n",
        "    naive_pd_e_list.append(naive_pd_edge(gditem, P_list[ind]))\n",
        "  naive_time = time.time() - naive_t\n",
        "  naive_time_list.append(naive_time)\n",
        "\n",
        "  # gudhi\n",
        "  gd_t = time.time()\n",
        "  for ind in range(0, len(data_list)):\n",
        "    gditem = data_list[ind]\n",
        "    gudhi_pd_e_list.append(gudhi_pd_edge(gditem, P_list[ind]))\n",
        "  gudhi_time = time.time() - gd_t\n",
        "  gudhi_time_list.append(gudhi_time)\n",
        "\n",
        "\n",
        "  print(dataset_names[i])\n",
        "  print(\"Theorem 5: \", thm5_time)\n",
        "  print(\"Union Find: \", naive_time)\n",
        "  print(\"gudhi: \", gudhi_time)\n",
        "  print(\"-------------------------\")"
      ],
      "metadata": {
        "id": "pMwDkwH9ULKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56638e05-e458-45cd-925b-19d30355f891"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic\n",
            "Theorem 5:  0.8054904937744141\n",
            "Union Find:  2.7030720710754395\n",
            "gudhi:  0.11327505111694336\n",
            "-------------------------\n",
            "str\n",
            "Theorem 5:  2.2883822917938232\n",
            "Union Find:  32.799060583114624\n",
            "gudhi:  3.503389596939087\n",
            "-------------------------\n",
            "dr\n",
            "Theorem 5:  0.3792917728424072\n",
            "Union Find:  22.13980793952942\n",
            "gudhi:  1.7820289134979248\n",
            "-------------------------\n",
            "4vtx\n",
            "Theorem 5:  1.3597157001495361\n",
            "Union Find:  149.3400056362152\n",
            "gudhi:  42.74673390388489\n",
            "-------------------------\n",
            "regular\n",
            "Theorem 5:  0.19175100326538086\n",
            "Union Find:  0.9566614627838135\n",
            "gudhi:  0.05453777313232422\n",
            "-------------------------\n",
            "extension\n",
            "Theorem 5:  0.4298710823059082\n",
            "Union Find:  2.8201935291290283\n",
            "gudhi:  0.1920773983001709\n",
            "-------------------------\n",
            "cfi\n",
            "Theorem 5:  5.464076519012451\n",
            "Union Find:  780.0260004997253\n",
            "gudhi:  33.380260705947876\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions to check if two persistence diagrams are equal\n",
        "def pd_to_multiset(pd_list):\n",
        "  output = {}\n",
        "  for item in pd_list:\n",
        "    output[str(item)] = 0\n",
        "  for item in pd_list:\n",
        "    output[str(item)] += 1\n",
        "  return output\n",
        "\n",
        "def check_pd_equal(pd1, pd2):\n",
        "  pd1_list = pd_to_multiset(pd1)\n",
        "  pd2_list = pd_to_multiset(pd2)\n",
        "\n",
        "  return pd1_list, pd2_list, pd1_list == pd2_list"
      ],
      "metadata": {
        "id": "OXNWZQ7r1sky"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking that the outputs match each other\n",
        "for i in range(0, len(thm_5_pd_list)):\n",
        "  thm4_output = thm_5_pd_list[i]\n",
        "  naive_output = naive_pd_e_list[i]\n",
        "  gudhi_output = gudhi_pd_e_list[i]\n",
        "\n",
        "  left_list, middle_list, value = check_pd_equal(thm4_output, naive_output)\n",
        "  middle_list, right_list, value1 = check_pd_equal(naive_output, gudhi_output)\n",
        "\n",
        "  if not (value and value1):\n",
        "    print(\"False: \", i)\n",
        "    print(\"Thm4 = Naive\", value)\n",
        "    print(\"Naive = gudhi\", value1)\n",
        "    print(left_list)\n",
        "    print(middle_list)\n",
        "    print(right_list)\n",
        "    print(\"--------------------------------------------\")"
      ],
      "metadata": {
        "id": "GTTdIOEg2vqc"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}